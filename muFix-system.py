#!/usr/bin/env python
# coding: utf-8

# In[1]:


AZURE_OPENAI_VERSION = '2023-03-15-preview'
AZURE_OPENAI_DEPLOYMENT= "llmops-model-amrit"
AZURE_OPENAI_ENDPOINT = 'https://llmops-amrit.openai.azure.com/'
AZURE_OPENAI_KEY= '4bad95f230d24dfcb400e2783939f4b1'

import os

os.environ["AZURE_OPENAI_API_KEY"] = AZURE_OPENAI_KEY
os.environ["AZURE_OPENAI_ENDPOINT"] = AZURE_OPENAI_ENDPOINT
os.environ["AZURE_OPENAI_API_VERSION"] = AZURE_OPENAI_VERSION
os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = AZURE_OPENAI_DEPLOYMENT
from langchain_core.messages import HumanMessage
from langchain_openai import AzureChatOpenAI

model = AzureChatOpenAI(
    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
    azure_deployment=os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"],
    temperature = 0
)


# In[2]:


from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.utils.function_calling import convert_to_openai_tool


# In[83]:


from datasets import load_dataset


# ### Retrieved Problems class and Plan class

# In[3]:


from typing import List
from langchain.output_parsers import PydanticOutputParser


class RetrievedProblem(BaseModel):
    description: str = Field(description= "Description of retrieved problem")
    code: str = Field(description="Code block with code to solve retrieved problem")
    planning: str = Field(description="Detailed plan and approach to solve retrieved problem")
    algorithm: str = Field(description="""Algorithm identified to solve retrieved problem
                         along with high-level generic tutorial to solve that type of problem""")
        

        
class Exemplar(BaseModel):
    """List of relevant problems"""
    problems:List[RetrievedProblem] = Field(description="List of relevant problems and solutions")


class Plan(BaseModel):
    """
    Plan to solve the original problem with associated confidence score
    """
    plan_description: str = Field(description="Description of plan")
    confidence_score: float = Field(description="Confidence score for plan to solve original problem")
        
parser = PydanticOutputParser(pydantic_object=Exemplar)


# ### Graph State

# In[4]:


from typing import List, TypedDict
from langgraph.graph import StateGraph


class GraphState(TypedDict):
    """
    Represents the state of our graph.
    """

    problem: str = Field(description="The original problem which is to be solved")
    test_case_analysis: str = Field(description="Analysis of test cases")
    expected_output:str = Field(description="The correct output for the sample test cases")
    inferred_output: str = Field(description="Inferred output from test case analysis")
    test_case_analysis_iterations: int = Field(description="Tracks number of iterations made to test-case analysis")
    correct_understanding: bool = Field(description="Flag indicating whether test-case analysis is correct")
    relevant_problems: Exemplar = Field("Examples retrived by the retrival agent")
    plans: List[Plan] = Field("List of plans to solve the original problem with associated confidence score")
    cur_plan: int = Field(description="ID of the current plan being sent to coding agent")
    generated_code: str = Field(description="Code generated to solve original problem")
    code_exec_result: dict = Field(description="Result of code execution containing errors, if any")
    modified_plan: str = Field(description="Stores the modified plan generated by debugging agent")
    debug_iterations: List[int] = Field(description="Tracks the iteration count for each plan")
    taken_feedback: bool = Field(description="Flag indicating whether user input was taken")
    user_feedback: str = Field(description="Stores feedback from user guiding the system towards the right direction")


# In[5]:


import re
def extract_sample_io(problem: str):
    input_match = re.search(r"-----Example-----\s*Input:\s+(.*?)\s*Output:\s", problem, re.DOTALL)
    sample_input = input_match.group(1).strip() if input_match else ""
    
    output_match = re.search(r"Output:\s+(.*?)\s*-----Note-----", problem, re.DOTALL)
    sample_output = output_match.group(1).strip() if output_match else ""
    return sample_input, sample_output


# In[6]:


question = """
You have $n$ barrels lined up in a row, numbered from left to right from one. Initially, the $i$-th barrel contains $a_i$
liters of water. You can pour water from one barrel to another. In one act of pouring, you can choose two different
barrels $x$ and $y$ (the $x$-th barrel shouldn't be empty) and pour any possible amount of water from barrel $x$ to barrel $y$
(possibly, all water). You may assume that barrels have infinite capacity, so you can pour any amount of water in each of them.
Calculate the maximum possible difference between the maximum and the minimum amount of water in the barrels, if you can pour
water at most $k$ times. Some examples: if you have four barrels, each containing $5$ liters of water, and $k = 1$, you may
pour $5$ liters from the second barrel into the fourth, so the amounts of water in the barrels are $[5, 0, 5, 10]$, and the
difference between the maximum and the minimum is $10$; if all barrels are empty, you can't make any operation, so the
difference between the maximum and the minimum amount is still $0$.
-----Input----- The first line contains one integer $t$ ($1 \le t \le 1000$) — the number of test cases.
The first line of each test case contains two integers $n$ and $k$ ($1 \le k < n \le 2 \cdot 10^5$) — the number of barrels
and the number of pourings you can make. The second line contains $n$ integers $a_1, a_2, \dots, a_n$ ($0 \le a_i \le 10^{9}$),
where $a_i$ is the initial amount of water the $i$-th barrel has. It's guaranteed that the total sum of $n$ over test cases
doesn't exceed $2 \cdot 10^5$.
-----Output----- For each test case, print the maximum possible difference between the maximum and the minimum amount of
water in the barrels, if you can pour water at most $k$ times.
-----Example-----
Input:
2
4 1
5 5 5 5
3 2
0 0 0
Output:
10
0
"""
#-----Explanation:----- Test Case 1: Chef starts the journey from the $N = 0$ at time $t = 0$ and it's the first time $(K = 1)$, he is here. So, the answer is $0$. Test Case 2: Chef starts the journey from the $N = 0$ at time $t = 0$ then goes to $N = 1$ at $t = 1$ and it's the first time $(K = 1)$, he is here. So, the answer is $1$. Test Case 4: The path followed by Chef to reach $1$ for the third time is given below. $0 - 1 - 0 - 1 - 2 - 1$ He reaches $1$ for the third time at $t=5$.
#Explanation: You could form "10", but then you'd have nothing left. Better form "0" and "1".
#Explanation: This are totally 4 strings can be formed by the using of 5 0s and 3 1s, which are “10,”0001”,”1”,”0”
#-----EXPLANATION:----- In the first sample the pizza is already cut into four equal slices. In the second sample the pizza will be cut into three equal slices after making one extra cut at $330$ degrees. In the third sample Vasya will have to cut his pizza into $360$ pieces of $1$ degree angle each.


# ### Initial Understanding Generation (Test-case analysis) 

# In[8]:


def run_test_case_analysis(state:GraphState):
    print("-----GENERATING INITIAL TEST CASE ANALYSIS-----")
    original_problem = state['problem']
#     original_problem = question
    test_case_analysis_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are a professional programmer proficient in analysing test-cases.
                You are provided a problem, and its associated test case(s) comprising input and output.
                Provide an analysis on how the given input leads to the given output.
                
                Let's analyze the test case(s) step by step. Your response MUST adhere to the following
                structure/format, fill in all the '<?>'s only with the required info for each test case:
                Note:
                - Put all the analysis in the '<?>' after analysis only. 
                - Mention the output only where needed in the response schema.
                - And also do not use curly brackets anywhere in your response
                
                Problem : {original_problem}
                
                Response structure/format:
                <?> The input is: <?>. The output is <?>. Analysis: <?>. Therefore, the expected
                output is <?>
                """
            )
        ]
    )
    test_case_analysis_chain = test_case_analysis_prompt | model
    initial_understanding = test_case_analysis_chain.invoke({'original_problem': original_problem})
#     return initial_understanding.content
    state['test_case_analysis'] = initial_understanding.content
    expected_outputs = extract_outputs(initial_understanding.content)
    expected_outputs = [s.strip('"') for s in expected_outputs]
    state['expected_output'] = expected_outputs
    print(initial_understanding.content)
    return state


# In[11]:


r = run_test_case_analysis()
print(r)


# In[5]:


def mask_output(text):
    pattern = re.compile(r'output is [^.\n]*')
    masked_text = pattern.sub('output is <?>', text)
    return masked_text
def extract_expected_output(text):
    # Define the regex pattern to match the specific expected output
    pattern = re.compile(r'Therefore, the expected output is ([^.\n]*)')
    
    # Find all matches in the input text
    matches = pattern.findall(text)
    
    return matches


# In[46]:


masked = mask_output(r)
print(masked)


# ### Infer output from test-case analysis

# In[9]:


def correctness_checking(state:GraphState):
    print("-----INFERRING OUTPUT FROM TEST CASE ANALYSIS-----")
    #     initial_understanding = r
    #     original_problem = question
    
    initial_understanding = state['test_case_analysis']
    original_problem = state['problem']
    
    masked_initial_understanding = mask_output(initial_understanding)
    print("MASKED INITAL UNDERSTANDING --- ")
    print(masked_initial_understanding)
    problem_without_testcase = extract_problem_without_testcase(original_problem)
    print("PROBLEM WITHOUT TESTCASE----")
    print(problem_without_testcase)
    
    correctness_checking_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system"
                """
                Problem Description: {problem_without_testcase}
                
                Initial understanding: {masked_initial_understanding}
                
                # Instructions:
                You have been provided a coding problem and an understanding of its logic.
                The understanding comprises of a test case analysis where for each test case you are given:
                - The input
                - The analysis, which mentions how the given input leads to the output.
                Your task is to follow the given analysis, and infer the expected output for the test-case(s).
                
                Your response MUST adhere to the format below only, fill in the <?>s with the required information:
                
                Response format: <?> Therefore, the expected output is <?>
                
                """
            )
        ]
    ).partial(problem_without_testcase=problem_without_testcase)
    correctness_checking_chain = correctness_checking_prompt | model
    inferred_output = correctness_checking_chain.invoke({"masked_initial_understanding":masked_initial_understanding})
#     return inferred_output.content
    state['inferred_output'] = inferred_output.content
    print(inferred_output.content)
    
    expected_outputs = state['expected_output']
    inferred_outputs = extract_outputs(inferred_output.content)
    
    print(f"expected outputs: {expected_outputs}")
    print(f"inferred outputs: {inferred_outputs}")
    
    if len(expected_outputs) != len(inferred_outputs):
        state['correct_understanding'] = False
        return state
    for i in range(len(expected_outputs)):
        if expected_outputs[i] != inferred_outputs[i]:
            state['correct_understanding'] = False
            return state
        
    state['correct_understanding'] = True
    return state


# In[49]:


x = """
For Test Case 1, the expected output is fzfsirk.
For Test Case 2, the expected output is xxxxxx.
For Test Case 3, the expected output is oio.
"""
print(extract_outputs(x))


# In[50]:


y = """
# Test Case 1
Input:
tinkoff
zscoder
Output:
fzfsirk

<?>
The input is: "tinkoff", "zscoder". The output is "fzfsirk".
Initially, the company name is ???????. Oleg replaces the first question mark with 'f'. The company name becomes f??????.
Igor replaces the second question mark with 'z'. The company name becomes fz?????.
Oleg replaces the third question mark with 'f'. The company name becomes fzf????.
Igor replaces the fourth question mark with 's'. The company name becomes fzfs???.
Oleg replaces the fifth question mark with 'i'. The company name becomes fzfsi??.
Igor replaces the sixth question mark with 'r'. The company name becomes fzfsir?.
Oleg replaces the seventh question mark with 'k'. The company name becomes fzfsirk.
Therefore, the expected output is "fzfsirk".

# Test Case 2
Input:
xxxxxx
xxxxxx
Output:
xxxxxx

<?>
The input is: "xxxxxx", "xxxxxx". The output is "xxxxxx".
No matter how they play, the company name will always be xxxxxx.
Therefore, the expected output is "xxxxxx".

# Test Case 3
Input:
ioi
imo
Output:
fzfsirk

<?>
The input is: "ioi", "imo". The output is "fzfsirk".
Initially, the company name is ???. Oleg replaces the second question mark with 'i'. The company name becomes ?i?.
The set of letters Oleg have now is {i, o}. Igor replaces the third question mark with 'o'. The company name becomes ?io.
The set of letters Igor have now is {i, m}. Finally, Oleg replaces the first question mark with 'o'. The company name becomes oio.
Therefore, the expected output is "oio".
"""
z = extract_outputs(y)
cleaned_array = [s.strip('"') for s in z]
print(cleaned_array)


# In[36]:


c = correctness_checking()
print(c)


# In[10]:


def extract_outputs(text):
    outputs = re.findall(r"the expected output is ([^.\n]*)", text)
    outputs = [output.strip() for output in outputs]
    return outputs


# ### Routing function to check correctness of inferred output 

# In[11]:


def check_inferred_output(state:GraphState):
    print("-----CHECKING CORRECTNESS OF INFERRED OUTPUT-----")
    correct_understanding = state['correct_understanding']
    if correct_understanding:
        print("-----DESICION: ACCURATE TEST CASE ANALYSIS FOUND-----")
        return "proceed"
    else:
        test_case_analysis_iterations = state['test_case_analysis_iterations']
        if test_case_analysis_iterations < 3:
            print("-----DESICION: MISUNDERSTANDING IN TEST CASE ANALYSIS-----")
            return "misunderstanding_present"
        else:
            print("-----DESICION: COULD NOT GENERATE ACCURATE TEST CASE ANALYSIS-----")
            return "proceed"
    
#     inferred_output = state['inferred_output']
#     test_case_analysis = state['test_case_analysis']
#     test_case_analysis_iterations = state['test_case_analysis_iterations']
    
#     expected_outputs = extract_outputs(test_case_analysis)
#     inferred_outputs = extract_outputs(inferred_output)
    
    
#     if len(expected_outputs) != len(inferred_outputs):
#         return "misunderstanding_present"
#     for i in range(len(expected_outputs)):
#         if expected_outputs[i] != inferred_outputs[i]:
#             if test_case_analysis_iterations >= 3:
#                 print("-----DESICION: COULD NOT GENERATE ACCURATE TEST CASE ANALYSIS-----")
#                 return "proceed"
#             print("-----DESICION: MISUNDERSTANDING IN TEST CASE ANALYSIS-----")
#             return "misunderstanding_present"
        
#     print("-----DESICION: ACCURATE TEST CASE ANALYSIS FOUND-----")
#     return "proceed"
    


# In[12]:


def extract_problem_without_testcase(text):
    pattern = re.compile(r'^(.*?)-----Example-----', re.DOTALL)
    match = pattern.search(text)
    if match:
        return match.group(1).strip()
    else:
        return "No match found"


# In[72]:


print(extract_problem_without_testcase(question))


# ### Fix misunderstanding 

# In[13]:


def misunderstanding_fixing(state:GraphState):
    print("-----DESICION: REFINING TEST CASE ANALYSIS-----")
    original_problem = state['problem']
    current_understanding = state['test_case_analysis']
    inferred_output = state['inferred_output']
    
#     original_problem = question
#     current_understanding = r
#     inferred_output = c
    misunderstanding_fixing_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are an advanced programmer, proficient in competitive coding and test-case analysis.
                You are provided a coding problem and an analysis of its test cases.
                This test case analysis was used (with the outputs masked) to infer the output from the input test cases,
                but it was incorrect in doing so, meaning that the initial test-case analysis was not accurate or
                descriptive enough. You are also provided with that inferred output.
                
                # Problem Specification: {original_problem}
                
                # Test Case Analysis: {current_understanding}
                
                # Inferred Output from understanding: {inferred_output}
                
                # Your task:
                Analyze why the current understanding(current test-case analysis) is not sufficient to infer the correct output for the test cases.
                Refine and update the understanding of the problem specification to produce the correct output for all the test cases, i.e. provide
                a refined analysis on how the given input leads to the given output provided in the original problem. Do so by
                carefully understanding the problem and breaking down its logic. The test-cases given are 100% correct, it is
                simply your task to correctly break down how the input leads to that output.
                And also do not use curly brackets anywhere in your response
                
                Let's analyze the test case(s) step by step. Your response MUST adhere to the following
                structure/format, fill in all the '<?>'s only with the required info for each test case:
                
                Response Schema:
                <?> The input is <?>. The output is <?>. Analysis: <?>. Therefore, the expected
                output is <?>
                """
            )
        ]
    )
    misunderstanding_fixing_prompt_formatted = misunderstanding_fixing_prompt.partial(
        current_understanding=current_understanding,
        inferred_output=inferred_output
    )
    misunderstanding_fixing_chain = misunderstanding_fixing_prompt_formatted | model
    fixed_anaysis = misunderstanding_fixing_chain.invoke({"original_problem":original_problem})
#     return fixed_anaysis.content
    print(fixed_anaysis.content)
    state['test_case_analysis'] = fixed_anaysis.content
    state['test_case_analysis_iterations'] += 1
    return state


# In[110]:


mist = misunderstanding_fixing()
print(mist)


# ### Retrieval Agent

# In[14]:


def retrieval_agent(state: GraphState) -> GraphState:
    print("-----RETRIEVING RELEVANT PROBLEMS-----")
    original_problem = state['problem']
    retrieval_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are a retrieval agent. Your task is to assist in solving
                competitive programming problems by recalling similar relevant problems,
                identifying the underlying algorithm, and providing step-by-step solutions
                and detailed plans for each problem. Follow the instructions carefully and provide
                your response in the specified format.

                Here is the question:{question}

                Task instructions:
                1. Recall 3 relevant and distinct problems (different from the problem given above)
                    - They must not be a rephrasing of the original problem or a slight modification of it.
                    - You must ensure each problem is distinct and highlights a different aspect of the approach
                    needed to solve the original problem. Do NOT repeat any problems.
                    - The problems must be relevant and helpful towards solving the original problem.
                2. For each problem :
                    - Provide a description of the problem.
                    - Generate python code to solve that problem.
                    - Finally, generate a plan to solve that problem
                    - Identify the algorithm (Brute-force, Dynamic Programming, 
                    Divide-and-conquer, Greedy, Backtracking, Recursive, Binary search, and so on) 
                    behind it that can be used to solve the original problem and provide a high-level tutorial about it.

                Your response MUST be based on this schema:
                {format_instructions}

                Always abide by the above mentioned schema. Follow these instructions carefully and I will give you 1000 Euros.

                """
            ),
    #         ("placeholder", "{messages}"),
        ]
    ).partial(format_instructions = parser.get_format_instructions())
    
    retrieval_chain = retrieval_prompt | model | parser
    
    problems = retrieval_chain.invoke({'question': original_problem})
    
    state['relevant_problems'] = problems
    
    return state


# ### Planning Agent

# In[15]:


def planning_agent(state: GraphState) -> GraphState:
    print("-----GENERATING PLANS-----")
    original_problem = state['problem']
    relevant_problems = state['relevant_problems']
    correct_understanding = state['correct_understanding']
    list_of_plans: List[Plan] = []
        
    base_prompt = """
            You are an advanced planning agent designed to create detailed
            step-by-step plans for solving competitive programming problems.
            You have been provided with a self-retrieved example problem and
            its associated solution. Your task is to generate a concrete plan that includes
            every necessary step to solve a new, original problem. The plan should incorporate
            insights from the provided example. Provide the plan in sequential steps.

            Relevant problem:
            {description}

            Planning:
            {planning}

            Relevant algorithm: {algorithm}

            Problem to be solved:{original_problem}
    """
    if correct_understanding:
        test_case_analysis = state['test_case_analysis']
        formatted_planning = f"""
        Analysis of the test cases: {test_case_analysis}
        
        Use the provided test case analysis to help guide your planning process
        """
        base_prompt += f"\n{formatted_planning}"
        
    further_instructions = """
        Make sure to 
            - describe in detail how to handle inputs for the problem
            - Analyze the problem to be solved carefully and do not make any logical errors
            - break down the core logic of the problem, detailing each step
            - and also identify potential edge cases and how to handle them.
            Respond with only the planning to solve the problem. Do not write any code.
            Use a Chain-Of-Thought sequence to break down the steps required to solve the problem.
            If you follow all these instructions carefully, I will give you 1000 Euros.
    """
    base_prompt += further_instructions
    
    planning_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                base_prompt
            )
        ]
    )
    
    confidence_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are an advanced evaluation agent designed to assess the quality of
                plans for solving competitive programming problems. Your task is to evaluate
                the provided plan for solving the given problem and assign a confidence score
                based on its completeness, correctness, logical soundness and efficiency.
                
                Your instructions:
                1. Review the original problem description
                2. Review the provided plan
                3. Assign a confidence score to the plan between 0 and 100, where 100 indicates
                complete confidence in the plan's effectiveness.
                
                This is the original problem:
                {problem_description}
                
                Provided plan:
                {plan}
                
                Respond with just the confidence score. Do not provide any explanation or add
                any other words.
                """
            )
        ]
    )
    
    for problem_number, problem in enumerate(relevant_problems.problems):
        planning_prompt_formatted = planning_prompt.partial(
            description=problem.description,
            planning=problem.planning,
            algorithm=problem.algorithm,
        )
        print(planning_prompt_formatted)
        planning_chain = planning_prompt_formatted | model
        the_plan = planning_chain.invoke({'original_problem': original_problem})
        
        confidence_prompt_formatted = confidence_prompt.partial(
            plan=the_plan.content,
        )
        confidence_gen_chain = confidence_prompt_formatted | model
        confidence_score = confidence_gen_chain.invoke({'problem_description':original_problem})
        
        generated_plan = Plan(
            plan_description = the_plan.content,
            confidence_score = confidence_score.content
        )
        
        list_of_plans.append(generated_plan)
        
    list_of_plans = sorted(list_of_plans, key=lambda x: x.confidence_score, reverse = True)
    state['plans'] = list_of_plans
    return state


# ### Coding Agent 

# In[16]:


def coding_agent(state: GraphState) -> GraphState:
    cur_plan = state['cur_plan']
    print(f"-----GENERATING CODE FOR PLAN {cur_plan + 1}-----")
    plans = state['plans']
    current_plan = plans[cur_plan]
    original_problem = state['problem']
    debug_iterations = state['debug_iterations']
    
    routed_from_debugger = False
    
    #check if being re-routed from debugging agent
    if (debug_iterations[cur_plan] > 0):
        routed_from_debugger = True
        modified_plan = state['modified_plan']
    
    
    coding_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                You are an advanced coding agent designed to convert detailed plans
                into executable python code. You have been provided with a problem, and a
                plan to solve that problem. Your task is to write the code to solve
                the original problem based on the plan.
                
                Plan to solve the problem:
                {plan}
                
                Original problem: 
                {original_problem}
                
                Lets think step by step.
                Respond with the code only. Do not add any extra explanation or words
                """
            )
        ]
    )
    
    if (routed_from_debugger):
        # format coding_prompt with new plan
        coding_prompt_formatted = coding_prompt.partial(
            plan = modified_plan
        )
    else:
        # need to implement logic to cycle through different plans if no. of iterations exceeds some limit
        coding_prompt_formatted = coding_prompt.partial(
            plan=current_plan.plan_description,
        )
        
    # print(coding_prompt_formatted)
    coding_chain = coding_prompt_formatted | model
        
    # print(coding_chain)
    code = coding_chain.invoke({'original_problem': original_problem})
    
#     test_input, test_output = extract_sample_io(hard_question)
#     result = test_code(code.content, test_input, test_output)

#     print("generated code")
#     print(code.content)
    
    state['generated_code'] = code.content
#     state['result'] = result
    
    return state


# ###  Function to extract sample IO 

# In[17]:


import re
def extract_sample_io(problem: str):
    input_match = re.search(r"-----Example-----\s*Input:\s+(.*?)\s*Output:\s", problem, re.DOTALL)
    sample_input = input_match.group(1).strip() if input_match else ""
    
    output_match = re.search(r"Output:\s+(.*?)\s*-----Note-----", problem, re.DOTALL)
    sample_output = output_match.group(1).strip() if output_match else ""
    return sample_input, sample_output


# ### Function to test code against sample IO 

# In[18]:


import sys
from io import StringIO
import traceback
def test_code(code: str, sample_input: str, sample_output: str):
    def mock_input():
        for line in sample_input.strip().split("\n"):
            yield line
    input_gen = mock_input()
    def input():
        return next(input_gen)
    
    original_stdin = sys.stdin
    original_stdout = sys.stdout
    sys.stdin = StringIO(sample_input.strip())
    sys.stdout = StringIO()
    
    namespace = {
        "input": input,
        "print": print,
        "map": map,
        "int": int,
        "range": range,
        # Any other necessary built-ins can be added here
    }
    execution_successful = False
    output_matches = False
    error_message = ""
    try:
        exec(code, namespace)
        output = sys.stdout.getvalue().strip()
        execution_successful = True
    except Exception as e:
        error_message = traceback.format_exc()
        output = f"Generated code has an error:{error_message}"
#         error_message = str(e)
        
    finally:
        sys.stdin = original_stdin
        sys.stdout = original_stdout
        
    expected_output = sample_output.strip()
    
    if execution_successful:
        output_matches = output==expected_output
            
    result ={
        "execution_successful":execution_successful,
        "output_matches":output_matches,
        "output":output,
        "expected_output":expected_output,
        "error_message":error_message
    }
    return result


# ### Executor Agent

# In[19]:


def executor_agent(state:GraphState)->GraphState:
    print("-----TESTING CODE AGAINST SAMPLE IO-----")
    code = state['generated_code']
    original_problem = state['problem']
    
    test_input, test_output = extract_sample_io(original_problem)
#     print("test input")
#     print(test_input)
#     print("test output")
#     print(test_output)
    
    exec_result = test_code(code, test_input, test_output)
    print(f"execution result: {exec_result}")
    
    state['code_exec_result'] = exec_result
    return state


# ### Router Function 

# In[20]:


def router_function(state:GraphState):
    # extract code execution results
    code_exec_result = state['code_exec_result']
    execution_successful = code_exec_result["execution_successful"]
    output_matches = code_exec_result["output_matches"]
    cur_plan = state['cur_plan']
    
    if execution_successful and output_matches:
        # generated code is successful, return to user
        print("-----DESICION FINISH-----")
        return "end"
    else:
        if state['debug_iterations'][cur_plan] >= 3:
            if state['taken_feedback']:
                print ("-----END: HUMAN FEEDBACK LOOP HAS ENDED-----")
                return "end"
            else:
                if cur_plan >= 2:
                    print("-----DESICION : Could not find working solution-----")
                    return "end"
                # select plan with next highest confidence and try again
                print("-----DESICION : TRY NEXT PLAN-----")
                return "next_plan"
        else:
            # go to debugging agent to try fixing code
            print("-----DESICION : TRY FIXING CODE-----")
            return "debug"


# ### Node to update current plan and reset debug iterations

# In[21]:


def next_plan(state:GraphState):
    state['cur_plan'] +=1 
#     state['debug_iterations'] = 0
    return state


# ### Node to take human input

# In[22]:


def human_feedback(state:GraphState):
    # graph has been interrputed right before reaching this node, state['user_feedback'] was just populated by user
    # cur_plan has also been updated with users choice
    state['taken_feedback'] = True
    cur_plan = state['cur_plan']
    state['debug_iterations'][cur_plan] = 0
    print("DEBUG ITERATIONS STATE RN")
    print(state['debug_iterations'])
    return state;


# ### Debugging Agent 

# In[23]:


def debugging_agent(state:GraphState)->GraphState:
    print("-----DEBUGGING-----")
    code_exec_result = state['code_exec_result']
    generated_code = state['generated_code']
    original_problem = state['problem']
    taken_feedback = state['taken_feedback']
    correct_understanding = state['correct_understanding']
    
    execution_successful = code_exec_result["execution_successful"]
    output_matches = code_exec_result["output_matches"]
    output = code_exec_result["output"]
    expected_output = code_exec_result["expected_output"]
    error_message = code_exec_result["error_message"]
    
    if (execution_successful):
        if(not output_matches):
            # test case didnt pass
            base_prompt = """
                Given a competitive programming problem, you have generated Python code to solve the problem. But the generated
                code did not pass sample test cases. Analyse the code carefully, check whether the logic is correct, whether it
                accounts for all edge cases.

                Your task is to provide a modified planning to solve the original problem. Analyse the problem, understand where
                the current code is incorrect, and generate a plan to solve the original problem. Make sure that you
                - Explain the plan in as much detail as possible.
                - Takes into account all edge cases
                - Break down the logic in a clear manner.
                - Do not make any logical errors.

                Respond with just the plan, do not write any code. If you follow the steps correctly, I will give you 1000 Euros.

                Original Problem: {original_problem}
                Code: {generated_code}
                Expected output: {expected_output}
                Actual output: {output}
            """
            if correct_understanding:
                test_case_analysis = state['test_case_analysis']
                formatted_planning = f"""
                Analysis of the test cases: {test_case_analysis}
        
                Use the provided test case analysis to help guide your planning process
                """
                base_prompt += f"\n{formatted_planning}"
            
            if taken_feedback:
                user_feedback = state['user_feedback']
                formatted_feedback = f"""The user has provided some additional information to guide your debugging process.
                Incorporate it into your planning process.
                User input: {user_feedback}
                """
                base_prompt += f"\n{formatted_feedback}"
            debugging_prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        base_prompt
                    )
                ]
            )
            debugging_prompt_formatted = debugging_prompt.partial(
                generated_code=generated_code,
                expected_output=expected_output,
                output=output,
            )
    else:
        # code execution error
        base_prompt = """
        You are an advanced debugging specialist.
        Given a competitive programming, you have generated Python code to solve the problem. But the generated code
        faced an error upon execution. You are provided with the error message. Identify what the error is and 
        generate a new plan to solve the problem that doesn't encounter the same error, or any other errors.
                    
        Make sure that the plan is
                - Explained in detail step-by-step
                - Takes into account all edge cases
                - Break down the logic to solve the problem
                - Do not make any logical errors.

        Respond with just the plan and mention why the error occured.
        If you follow the steps correctly, I will give you 1000 Euros.
                    
        Original Question: {original_problem}
        Code: {generated_code}
        Error message: {error_message}
        """
        if correct_understanding:
            test_case_analysis = state['test_case_analysis']
            formatted_planning = f"""
            Analysis of the test cases: {test_case_analysis}
        
            Use the provided test case analysis to help guide your planning process
            """
            base_prompt += f"\n{formatted_planning}"
        
        if taken_feedback:
            user_feedback = state['user_feedback']
            formatted_feedback = f"""\nThe user has provided some additional information to guide your debugging process.
            Incorporate it into your planning process.
            User input: {user_feedback}
            """
            base_prompt += f"\n{formatted_feedback}"
        debugging_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    base_prompt
                )
            ]
        )
        debugging_prompt_formatted = debugging_prompt.partial(
            generated_code=generated_code,
            error_message=error_message,
        )
    
    debugging_chain = debugging_prompt_formatted | model
    modified_plan = debugging_chain.invoke({'original_problem': original_problem})
    state['modified_plan'] = modified_plan.content
    cur_plan = state['cur_plan']
    state['debug_iterations'][cur_plan] += 1
    return state


# ### Creating the Graph     

# In[24]:


from langgraph.graph import END, START
from langgraph.checkpoint.memory import MemorySaver

workflow = StateGraph(GraphState)

# defining the nodes
workflow.add_node("initial_test_case_analysis", run_test_case_analysis)
workflow.add_node("correctness_checking", correctness_checking)
workflow.add_node("misunderstanding_fixing", misunderstanding_fixing)
workflow.add_node("retrieval_agent", retrieval_agent)
workflow.add_node("planning_agent", planning_agent)
workflow.add_node("coding_agent", coding_agent)
workflow.add_node("executor_agent", executor_agent)
workflow.add_node("next_plan", next_plan)
workflow.add_node("debugging_agent", debugging_agent)
# workflow.add_node("human_feedback", human_feedback)

# build graph
workflow.add_edge(START,"initial_test_case_analysis")
workflow.add_edge("initial_test_case_analysis", "correctness_checking")
workflow.add_conditional_edges(
    "correctness_checking",
    check_inferred_output,
    {
        "misunderstanding_present":"misunderstanding_fixing",
        "proceed":"retrieval_agent"
    }
    
)
workflow.add_edge("misunderstanding_fixing", "correctness_checking")
workflow.add_edge("retrieval_agent", "planning_agent")
workflow.add_edge("planning_agent", "coding_agent")
workflow.add_edge("coding_agent", "executor_agent")
workflow.add_conditional_edges(
    "executor_agent",
    router_function,
    {
        "end": END,
        "next_plan": "next_plan",
        "debug": "debugging_agent",
#         "human_feedback": "human_feedback"
    }
)
workflow.add_edge("next_plan", "coding_agent")
# workflow.add_edge("human_feedback", "eval_debugging_agent")
workflow.add_edge("debugging_agent", "coding_agent")

memory = MemorySaver()

app = workflow.compile()


# In[25]:


from IPython.display import Image, display

try:
    display(Image(app.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass


# In[26]:


from langchain_core.runnables.config import RunnableConfig
# config = RunnableConfig(recursion_limit=50)


# In[27]:


def create_initial_state(problem:str)->GraphState:
    return GraphState(
        problem=problem,
        test_case_analysis="",
        inferred_output="",
        test_case_analysis_iterations=0,
        relevant_problems=Exemplar(problems=[]), 
        plans=[],
        cur_plan=0,
        generated_code="",
        code_exec_result={},
        modified_plan="",
        debug_iterations=[0,0,0],
        taken_feedback=False,
        user_feedback="",
    )


# In[28]:


question = """
You play a computer game. In this game, you lead a party of $m$ heroes, and you have to clear a dungeon with $n$ monsters.
Each monster is characterized by its power $a_i$. Each hero is characterized by his power $p_i$ and endurance $s_i$. The heroes
clear the dungeon day by day. In the beginning of each day, you choose a hero (exactly one) who is going to enter the dungeon
this day. When the hero enters the dungeon, he is challenged by the first monster which was not defeated during the previous
days (so, if the heroes have already defeated $k$ monsters, the hero fights with the monster $k + 1$). When the hero fights
the monster, there are two possible outcomes: if the monster's power is strictly greater than the hero's power, the hero
retreats from the dungeon. The current day ends; otherwise, the monster is defeated. After defeating a monster, the hero
either continues fighting with the next monster or leaves the dungeon. He leaves the dungeon either if he has already defeated
the number of monsters equal to his endurance during this day (so, the $i$-th hero cannot defeat more than $s_i$ monsters 
during each day), or if all monsters are defeated — otherwise, he fights with the next monster. When the hero leaves the
dungeon, the current day ends. Your goal is to defeat the last monster. What is the minimum number of days that you need to
achieve your goal? Each day you have to use exactly one hero; it is possible that some heroes don't fight the monsters at all.
Each hero can be used arbitrary number of times.
-----Input----- The first line contains one integer $t$ ($1 \le t \le 10^5$) — the number of test cases. Then the test
cases follow. The first line of each test case contains one integer $n$ ($1 \le n \le 2 \cdot 10^5$) — the number of monsters
in the dungeon. The second line contains $n$ integers $a_1$, $a_2$, ..., $a_n$ ($1 \le a_i \le 10^9$), where $a_i$ is the power
of the $i$-th monster. The third line contains one integer $m$ ($1 \le m \le 2 \cdot 10^5$) — the number of heroes in
your party. Then $m$ lines follow, each describing a hero.
Each line contains two integers $p_i$ and $s_i$ ($1 \le p_i \le 10^9$, $1 \le s_i \le n$) — the power and the endurance of
the $i$-th hero. It is guaranteed that the sum of $n + m$ over all test cases does not exceed $2 \cdot 10^5$.
-----Output----- For each test case print one integer — the minimum number of days you have to spend to defeat all of
the monsters (or $-1$ if it is impossible)
-----Example-----
Input:
2
6
2 3 11 14 1 8
2
3 2
100 1
5
3 5 100 2 3
2
30 5
90 1
Output:
5
-1
-----Note-----
"""
config = RunnableConfig(
    recursion_limit=55,  # Set your desired recursion limit
    configurable={"thread_id": "127"}  # Set your thread ID
)

initial_state = create_initial_state(question)

result = app.invoke(initial_state, config)
# execution_result = result['code_exec_result']
# success = False
# if execution_result['execution_successful'] and execution_result['output_matches']:
#     print("Working solution was found.")
#     success = True
# else:
#     state_history = list(app.get_state_history(config))
#     all_states = []
#     relevant_states =[]
#     relevant_state_ids = []
#     for i, state in enumerate(state_history):
#         all_states.append(state)
#         if i<len(state_history)-1 and i != 0 :
#             current_plan = state.values['cur_plan']
#             next_plan = state_history[i+1].values['cur_plan']
#             if current_plan is not None and next_plan is not None:
#                 if current_plan == next_plan-1:
#                     relevant_states.append(state)
#     relevant_states.append(app.get_state(config))
#     print("*" * 15)
#     print("Report of explored options:")
    
#     for i, state in enumerate(relevant_states):
#         state_values = state.values
#         print(f"Path {i+1}-")
#         print(f"Modified Plan {i+1} after debugging:")
#         print(state_values['modified_plan'])
#         print(f"Confidence score: {state_values['plans'][i].confidence_score}")
#         print(f"Code execution result: {state_values['code_exec_result']}")
#         print("*" * 15)


# In[29]:


result['generated_code']


# In[460]:


print(relevant_states)


# In[130]:


print(result['plans'][0].plan_description)


# In[78]:


if not success:
    user_plan_choice = int(input("Which plan seemed closest to the solution?: "))
    user_feedback = input("Please provide some feedback to assist in the debugging process")
    relevant_state_config = relevant_states[user_plan_choice-1].config
#     print(relevant_state_config)
    debug_iterations = [0 if i == user_plan_choice-1 else 3 for i in range(3)]
    branch_config = app.update_state(
        relevant_state_config,
        {
            "cur_plan": user_plan_choice-1,
            "user_feedback":user_feedback,
            "taken_feedback":True,
            "debug_iterations":debug_iterations
        }
    )
    print("STATE AFTER TAKING HUMAN INPUT")
    print(app.get_state(config).values)
    result = app.invoke(None, branch_config)


# In[ ]:




